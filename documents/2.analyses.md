# Audit du code et son impact business

## TL;DR

- Le temps de build est directement corrélé au temps de réponse des APIs externes, ce qui ajoute plusieurs secondes (voire minutes) d’attente à chaque build/start de développement ([détail](#1-apis-externes-bloquantes-pendant-le-build-sourcenodes)).

- Chaque changement de contenu déclenche un rebuild complet de toutes les pages WordPress, ce qui fait croître le temps de build avec le volume d’articles ([détail](#2-rebuild-complet-et-generation-systematique-de-toutes-les-pages-wordpress)).

- Les sitemaps et le flux RSS sont générés à la main, partiels et peu scalables, ce qui fragilise la couverture SEO et l’indexation à grande échelle ([détail](#3-limitation-et-fragilite-de-la-couche-seo-sitemaprss-volumetrie)).

## 1. APIs externes bloquantes pendant le build (`sourceNodes`)

- Le code récupère **tous les avis Trustpilot via une boucle `while (hasMore)`** et toutes les données de taux à chaque build, sans limite de pagination, sans timeout ni gestion d’erreur robuste.
- Les avis Trustpilot sont agrégés dans un tableau `allReviews` qui n’est **jamais utilisé par la suite** : on paie donc le coût complet de la pagination et des appels réseau, sans aucun bénéfice fonctionnel, ce qui rallonge encore inutilement le build et augmente la surface de panne.

- À l’échelle, **la durée du build dépend directement des performances de ces APIs externes** et de la taille des données (plus d’avis Trustpilot = plus d’appels = build plus lent). Si une API est lente ou down, le build peut échouer ou durer très longtemps, bloquant toute mise en production.

- **Impact business** : chaque modification de contenu ou besoin de preview déclenche un build qui peut être ralenti ou cassé par des services tiers hors de contrôle de l’équipe. L’équipe contenu se retrouve à attendre 15+ minutes, voire à ne pas pouvoir publier si l’API de taux ou Trustpilot est indisponible.

- En plus, les taux sont **d’abord fetchés puis transformés en nodes GraphQL (`Rate`) dans `sourceNodes`, puis relus intégralement via `allRate` dans `createPages` uniquement pour en prendre les 10 derniers**. Cela crée un **double passage coûteux et inutile** (fetch + GraphQL) pour un besoin très simple, alors qu’un simple fetch + cache local (ou passage direct dans le contexte) suffirait.

## 2. Rebuild complet et génération systématique de toutes les pages WordPress

- Dans `createPages`, le code **régénère toutes les pages d’articles WordPress** à chaque build en important la liste complète des posts (y compris le `content` complet, les catégories, le SEO, etc.), puis en créant une page par post.
- Il n’y a **aucune stratégie d’incrémentalité** : on ne se sert pas du champ `modified` pour ne reconstruire que les pages modifiées, et on recalcule même plusieurs fois des artefacts globaux (par ex. RSS dans `onPostBuild`) en refaisant des requêtes GraphQL complètes.
- À mesure que le nombre d’articles augmente (300, 1000, 1500+), **la durée du build croît linéairement avec le volume de contenu**, ce qui explique les ~15 minutes de build actuelles et leur augmentation dans le temps.
- **Impact business** : chaque nouvel article rend les builds plus lents, ce qui **dégrade durablement la productivité de l’équipe contenu** (attente avant mise en ligne, impossibilité de preview “instantané”), et allonge les délais de mise à jour du site en production.

## 3. Limitation et fragilité de la couche SEO (sitemap/RSS, volumétrie)

- La requête `allWpPost(limit: 1000)` dans `createPages` introduit une **limite artificielle** : au-delà de 1000 posts, une partie des contenus ne sera ni construite, ni listée dans le sitemap, ni potentiellement dans le RSS. Cela crée un **comportement non déterministe** quand le volume dépasse ce seuil.

- Le sitemap et le flux RSS sont générés **manuellement via des templates string** en boucle sur tous les posts à chaque build, sans pagination ni découpage. À grande échelle, cela devient coûteux en mémoire et en temps, et difficile à faire évoluer (ajout de nouveaux champs SEO, de nouveaux sitemaps, etc.).

- Le sitemap généré (`sitemap-posts.xml`) ne couvre **que les pages WordPress** et ignore les autres types de pages du site (pages institutionnelles, produits, etc.), ce qui donne **une vision incomplète du site aux moteurs de recherche**.
- Il n’existe pas de **sitemap index** (fichier racine listant plusieurs sitemaps spécialisés). Or les spécifications de Google prévoient **un maximum de 50 000 URLs ou 50 Mo non compressés par sitemap**, ce qui impose de découper proprement par type de contenu / volumétrie. Ici, un unique sitemap artisanal sera rapidement à la limite.

- Le sitemap est écrit directement pendant la phase de création de pages, ce qui n’est pas idéal : en mode développement, on ne souhaite généralement pas générer et maintenir des artefacts SEO finaux, alors qu’en production on veut un sitemap fiable post-build (idéalement géré par l’outillage de build ou un plugin dédié).

- Une approche plus robuste consisterait à **déléguer la génération des sitemaps et du flux RSS à un plugin ou à une couche dédiée** capable de gérer : index de sitemaps, pagination, couverture de tous les types de pages, limites de volumétrie, exécution uniquement au bon moment (post-build) et cohérence globale avec la stratégie SEO.

- **Impact business** : des articles potentiellement non inclus dans le sitemap ou le RSS signifient **risque de non-indexation ou de désindexation partielle**, donc perte de trafic organique. Couplé à des builds longs, cela crée un cercle vicieux où plus de contenu = plus de lenteur + SEO plus fragile.
